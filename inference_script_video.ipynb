{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb32a80-0e1b-4ddf-b2ff-1fe20611345b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e496b5c5-b8f1-40b3-821c-ed62af8f54ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import ast\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import cv2\n",
    "import neptune\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "from pytorch_faster_rcnn_tutorial.datasets import ObjectDetectionDatasetSingle, ObjectDetectionDataSet\n",
    "from pytorch_faster_rcnn_tutorial.faster_RCNN import get_faster_rcnn_resnet\n",
    "from pytorch_faster_rcnn_tutorial.transformations import ComposeDouble\n",
    "from pytorch_faster_rcnn_tutorial.transformations import ComposeSingle\n",
    "from pytorch_faster_rcnn_tutorial.transformations import FunctionWrapperDouble\n",
    "from pytorch_faster_rcnn_tutorial.transformations import FunctionWrapperSingle\n",
    "from pytorch_faster_rcnn_tutorial.transformations import apply_nms, apply_score_threshold\n",
    "from pytorch_faster_rcnn_tutorial.transformations import normalize_01\n",
    "from pytorch_faster_rcnn_tutorial.utils import get_filenames_of_path, collate_single, save_json\n",
    "from pytorch_faster_rcnn_tutorial.visual import DatasetViewer\n",
    "from pytorch_faster_rcnn_tutorial.visual import DatasetViewerSingle\n",
    "from pytorch_faster_rcnn_tutorial.backbone_resnet import ResNetBackbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f582ef-cf8b-4ddd-bfce-e986bfa6a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "params = {'EXPERIMENT': 'FRCNN-166',  # experiment name, e.g. Head-42\n",
    "          'OWNER': 'svf',  # e.g. johndoe55\n",
    "          'INPUT_DIR': 'pytorch_faster_rcnn_tutorial/data/stop_line/test_video',  # files to predict\n",
    "          # 'INPUT_DIR': 'pytorch_faster_rcnn_tutorial/data/heads/test',  # files to predict\n",
    "          'PREDICTIONS_PATH': 'pytorch_faster_rcnn_tutorial/data/stop_line/test_video/predictions',  # where to save the predictions\n",
    "          'MODEL_DIR': 'pytorch_faster_rcnn_tutorial\\data\\stop_line\\epoch=171-step=8771.ckpt',  # load model from checkpoint !epoch=275!306!\n",
    "          'DOWNLOAD': False,  # whether to download from neptune\n",
    "          'DOWNLOAD_PATH': 'model',  # where to save the model if DOWNLOAD is True\n",
    "          'PROJECT': 'F-RCNN',  # Project name\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f8bab4-464b-480b-883e-3e8e5e267ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "inputs = get_filenames_of_path(pathlib.Path(params['INPUT_DIR']))\n",
    "inputs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c30ad2-9fd9-4090-b300-78a36fe83ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transforms = ComposeSingle([\n",
    "    FunctionWrapperSingle(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperSingle(normalize_01)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f296e5-971c-40bc-a7ae-cdb3f110ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = ObjectDetectionDatasetSingle(inputs=inputs,\n",
    "                                       transform=transforms,\n",
    "                                       use_cache=False,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e02b562-62e3-42a1-abbd-892c3b77f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "dataloader_prediction = DataLoader(dataset=dataset,\n",
    "                                   batch_size=8,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=10,\n",
    "                                   collate_fn=collate_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe76faa4-35aa-4851-8a6d-91f601f425f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = os.environ['NEPTUNE']  # if this throws an error, you probably didn't set your env var\n",
    "api_key = os.environ.get('NEPTUNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ca9ab1-70c3-4d8c-895c-780e5b159120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#api_key = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3MjYwMmIwNy1jMDZlLTRiYTYtODQyZC0zNGRkMmRjYzg2MmUifQ==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "618796d3-1009-42ce-8fbc-4e8197a652ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import experiment from neptune\n",
    "project_name = f'{params[\"OWNER\"]}/{params[\"PROJECT\"]}'\n",
    "project = neptune.init(project_qualified_name=project_name, api_token=api_key)  # get project\n",
    "experiment_id = params['EXPERIMENT']  # experiment id\n",
    "experiment = project.get_experiments(id=experiment_id)[0]\n",
    "parameters = experiment.get_parameters()\n",
    "properties = experiment.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8042bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f455a449-db53-4e14-aa41-e2192ce719a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcnn transform\n",
    "transform = GeneralizedRCNNTransform(min_size=int(parameters['MIN_SIZE']),\n",
    "                                     max_size=int(parameters['MAX_SIZE']),\n",
    "                                     image_mean=ast.literal_eval(parameters['IMG_MEAN']),\n",
    "                                     image_std=ast.literal_eval(parameters['IMG_STD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abcea34a-aeb6-4ca2-9208-ac71074819b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataset\n",
    "# datasetviewer = DatasetViewerSingle(dataset, rccn_transform=None)\n",
    "# datasetviewer.napari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8277d891-0926-4005-938f-a8caf9646ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model from neptune or load from checkpoint\n",
    "if params['DOWNLOAD']:\n",
    "    download_path = pathlib.Path(os.getcwd()) / params['DOWNLOAD_PATH']\n",
    "    download_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_name = 'best_model.pt'  # that's how I called the best model\n",
    "    # model_name = properties['checkpoint_name']  # logged when called log_model_neptune()\n",
    "    if not (download_path / model_name).is_file():\n",
    "        experiment.download_artifact(path=model_name, destination_dir=download_path)  # download model\n",
    "\n",
    "    model_state_dict = torch.load(download_path / model_name, map_location=torch.device('cuda'))\n",
    "else:\n",
    "    checkpoint = torch.load(params['MODEL_DIR'], map_location=torch.device('cuda'))\n",
    "    model_state_dict = checkpoint['hyper_parameters']['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a78b32-2283-45ad-83c2-11337e4c2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!anchor_size: 1 cell_anchors: 1\n"
     ]
    }
   ],
   "source": [
    "# model init\n",
    "model = get_faster_rcnn_resnet(num_classes=int(parameters['CLASSES']),\n",
    "                               backbone_name=ResNetBackbones(parameters['BACKBONE'].split('.')[-1].lower()),  # reverse look-up enum\n",
    "                               anchor_size=ast.literal_eval(parameters['ANCHOR_SIZE']),\n",
    "                               aspect_ratios=ast.literal_eval(parameters['ASPECT_RATIOS']),\n",
    "                               fpn=ast.literal_eval(parameters['FPN']),\n",
    "                               min_size=int(parameters['MIN_SIZE']),\n",
    "                               max_size=int(parameters['MAX_SIZE'])\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d659f25-0d7d-4a97-be15-5795e24f2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(2560,), max_size=2561, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96a1d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 1.0000, 1.0000,  ..., 0.6902, 0.6902, 0.6902],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.6902, 0.6902, 0.6902],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.6902, 0.6902, 0.6902],\n",
      "          ...,\n",
      "          [0.3608, 0.3608, 0.3608,  ..., 0.0667, 0.0667, 0.0667],\n",
      "          [0.3647, 0.3647, 0.3647,  ..., 0.0667, 0.0667, 0.0667],\n",
      "          [0.3647, 0.3647, 0.3647,  ..., 0.0667, 0.0667, 0.0667]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 0.5333, 0.5333, 0.5333],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.5333, 0.5333, 0.5333],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.5333, 0.5333, 0.5333],\n",
      "          ...,\n",
      "          [0.3765, 0.3765, 0.3765,  ..., 0.0078, 0.0078, 0.0078],\n",
      "          [0.3804, 0.3804, 0.3804,  ..., 0.0078, 0.0078, 0.0078],\n",
      "          [0.3804, 0.3804, 0.3804,  ..., 0.0078, 0.0078, 0.0078]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 0.3843, 0.3843, 0.3843],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.3843, 0.3843, 0.3843],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.3843, 0.3843, 0.3843],\n",
      "          ...,\n",
      "          [0.3843, 0.3843, 0.3843,  ..., 0.0118, 0.0118, 0.0118],\n",
      "          [0.3882, 0.3882, 0.3882,  ..., 0.0118, 0.0118, 0.0118],\n",
      "          [0.3882, 0.3882, 0.3882,  ..., 0.0118, 0.0118, 0.0118]]]],\n",
      "       device='cuda:0') torch.Size([1, 3, 1440, 2560])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\iserg\\rebels_code\\FasterRCNN\\inference_script_video.ipynb Ячейка 17\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(frame\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)[np\u001b[39m.\u001b[39mnewaxis, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(tensor, tensor\u001b[39m.\u001b[39msize())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39minput\u001b[39;49m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     predict \u001b[39m=\u001b[39m model(tensor)\n",
      "File \u001b[1;32mc:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\ipykernel\\kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1192\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1195\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1196\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\ipykernel\\kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1232\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for frame_path in inputs:\n",
    "    input_video = cv2.VideoCapture(str(frame_path))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    save_dir = pathlib.Path(os.getcwd()) / params['PREDICTIONS_PATH']\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_movie = cv2.VideoWriter(os.path.join(save_dir, os.path.basename(frame_path)), fourcc, 30, (2560, 1440)) #(2560, 1440) (1920, 1080)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = input_video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        tensor = torch.from_numpy(frame.transpose(2, 0, 1)[np.newaxis, ...]/255).to(device=torch.device('cuda'), dtype=torch.float)\n",
    "        print(tensor, tensor.size())\n",
    "        input()\n",
    "        with torch.no_grad():\n",
    "            predict = model(tensor)\n",
    "            # print(predict[0])\n",
    "            # input()\n",
    "            if predict[0]['boxes'].numel():\n",
    "                pred_numpy = [{key: value.cpu().numpy()[0] for key, value in pr.items()} for pr in predict] #predict[0]['boxes'].cpu().numpy()[0]\n",
    "                if pred_numpy[0]['scores'] > 0.3:\n",
    "                    box = pred_numpy[0]['boxes']\n",
    "                    cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0,255,0), 2)\n",
    "\n",
    "        output_movie.write(frame)\n",
    "\n",
    "    output_movie.release()\n",
    "    print('file isOpened:', output_movie.isOpened())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e59d69e-b9ca-45b3-8320-2a7d4a0e4c6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\iserg\\rebels_code\\FasterRCNN\\pytorch_faster_rcnn_tutorial\\datasets.py\", line 171, in __getitem__\n    x = self.read_images(input_ID)\n  File \"c:\\Users\\iserg\\rebels_code\\FasterRCNN\\pytorch_faster_rcnn_tutorial\\datasets.py\", line 188, in read_images\n    return imread(inp)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\skimage\\io\\_io.py\", line 53, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\skimage\\io\\manage_plugins.py\", line 205, in call_plugin\n    return func(*args, **kwargs)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py\", line 15, in imread\n    return np.asarray(imageio_imread(*args, **kwargs))\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\imageio\\v2.py\", line 226, in imread\n    with imopen(uri, \"ri\", **imopen_args) as file:\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\imageio\\core\\imopen.py\", line 298, in imopen\n    raise err_type(err_msg)\nValueError: Could not find a backend to open `C:\\Users\\iserg\\rebels_code\\FasterRCNN\\pytorch_faster_rcnn_tutorial\\data\\stop_line\\test_video\\Stopline_ref.mp4`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  FFMPEG:  pip install imageio[ffmpeg]\n  pyav:  pip install imageio[pyav]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\iserg\\rebels_code\\FasterRCNN\\inference_script_video.ipynb Ячейка 18\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# inference (cpu)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m dataloader_prediction:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# print(\"-1-- %s seconds ---\" % (time.time() - start_time))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     x, x_names \u001b[39m=\u001b[39m sample\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     x \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m x]\n",
      "File \u001b[1;32mc:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1375\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1376\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1402\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1403\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\iserg\\rebels_code\\FasterRCNN\\pytorch_faster_rcnn_tutorial\\datasets.py\", line 171, in __getitem__\n    x = self.read_images(input_ID)\n  File \"c:\\Users\\iserg\\rebels_code\\FasterRCNN\\pytorch_faster_rcnn_tutorial\\datasets.py\", line 188, in read_images\n    return imread(inp)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\skimage\\io\\_io.py\", line 53, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\skimage\\io\\manage_plugins.py\", line 205, in call_plugin\n    return func(*args, **kwargs)\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py\", line 15, in imread\n    return np.asarray(imageio_imread(*args, **kwargs))\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\imageio\\v2.py\", line 226, in imread\n    with imopen(uri, \"ri\", **imopen_args) as file:\n  File \"c:\\Users\\iserg\\anaconda3\\envs\\Torch\\lib\\site-packages\\imageio\\core\\imopen.py\", line 298, in imopen\n    raise err_type(err_msg)\nValueError: Could not find a backend to open `C:\\Users\\iserg\\rebels_code\\FasterRCNN\\pytorch_faster_rcnn_tutorial\\data\\stop_line\\test_video\\Stopline_ref.mp4`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  FFMPEG:  pip install imageio[ffmpeg]\n  pyav:  pip install imageio[pyav]\n"
     ]
    }
   ],
   "source": [
    "# inference (cpu)\n",
    "model.eval()\n",
    "for sample in dataloader_prediction:\n",
    "    # print(\"-1-- %s seconds ---\" % (time.time() - start_time))\n",
    "    x, x_names = sample\n",
    "    x = [t.to(torch.device('cuda')) for t in x]\n",
    "    with torch.no_grad():\n",
    "        preds = model(x)\n",
    "        predicts = [{key: value.cpu().numpy() for key, value in pr.items()} for pr in preds]\n",
    "        names = [pathlib.Path(x_name) for x_name in x_names]\n",
    "        save_dir = pathlib.Path(os.getcwd()) / params['PREDICTIONS_PATH']\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for name, pred in zip(names, predicts):\n",
    "            pred_list = {key: value.tolist() for key, value in pred.items()}  # numpy arrays are not serializable -> .tolist()\n",
    "            save_json(pred_list, path=save_dir / name.with_suffix('.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2e424-4308-4586-a2ca-d0349cffc594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction files\n",
    "predictions = get_filenames_of_path(pathlib.Path(os.getcwd()) / params['PREDICTIONS_PATH'])\n",
    "predictions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66697a0b-7989-4e44-8056-b62b70a49bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\iserg\\rebels_code\\FasterRCNN\\inference_script_video.ipynb Ячейка 20\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m score_threshold \u001b[39m=\u001b[39m \u001b[39m0.45\u001b[39m \u001b[39m#0.6\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m transforms_prediction \u001b[39m=\u001b[39m ComposeDouble([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     FunctionWrapperDouble(np\u001b[39m.\u001b[39mmoveaxis, source\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, destination\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     FunctionWrapperDouble(normalize_01),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     FunctionWrapperDouble(apply_nms, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, target\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, iou_threshold\u001b[39m=\u001b[39miou_threshold),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     FunctionWrapperDouble(apply_score_threshold, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, target\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, score_threshold\u001b[39m=\u001b[39mscore_threshold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m dataset_prediction \u001b[39m=\u001b[39m ObjectDetectionDataSet(inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                             targets\u001b[39m=\u001b[39mpredictions,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                             transform\u001b[39m=\u001b[39mtransforms_prediction,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/iserg/rebels_code/FasterRCNN/inference_script_video.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                                             use_cache\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# create prediction dataset\n",
    "iou_threshold = 0.25 #0.25\n",
    "score_threshold = 0.45 #0.6\n",
    "\n",
    "transforms_prediction = ComposeDouble([\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01),\n",
    "    FunctionWrapperDouble(apply_nms, input=False, target=True, iou_threshold=iou_threshold),\n",
    "    FunctionWrapperDouble(apply_score_threshold, input=False, target=True, score_threshold=score_threshold)\n",
    "])\n",
    "\n",
    "dataset_prediction = ObjectDetectionDataSet(inputs=inputs,\n",
    "                                            targets=predictions,\n",
    "                                            transform=transforms_prediction,\n",
    "                                            use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ccb62e53-0e06-4bb5-86a3-8c23d1556ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping\n",
    "color_mapping = {\n",
    "    1: 'green',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65fc2b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_faster_rcnn_tutorial.visual.DatasetViewer at 0x1dee4574a90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetViewer(dataset_prediction, color_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff50e474-202d-48ac-b5d0-13d69c3b7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n",
    "datasetviewer_prediction = DatasetViewer(dataset_prediction, color_mapping)\n",
    "datasetviewer_prediction.napari()\n",
    "# add text properties gui\n",
    "datasetviewer_prediction.gui_text_properties(datasetviewer_prediction.shape_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b26ebc-1df1-4829-9e72-1bd2452976e7",
   "metadata": {},
   "source": [
    "## Experiment with Non-maximum suppression (nms) and score-thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b440cbf-94fa-439c-ac9f-134959617f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with nms and score-thresholding\n",
    "transforms_prediction = ComposeDouble([\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "dataset_prediction = ObjectDetectionDataSet(inputs=inputs,\n",
    "                                            targets=predictions,\n",
    "                                            transform=transforms_prediction,\n",
    "                                            use_cache=False)\n",
    "\n",
    "color_mapping = {\n",
    "    1: 'green',\n",
    "}\n",
    "\n",
    "datasetviewer_prediction = DatasetViewer(dataset_prediction, color_mapping)\n",
    "datasetviewer_prediction.napari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fde50455-2b3b-4530-bf12-fbade6e19bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add score slider\n",
    "# DOES CURRENTLY NOT WORK\n",
    "# datasetviewer_prediction.gui_score_slider(datasetviewer_prediction.shape_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace35a64-c53d-40f5-941e-485e085746c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nms slider\n",
    "# DOES CURRENTLY NOT WORK\n",
    "# datasetviewer_prediction.gui_nms_slider(datasetviewer_prediction.shape_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d6cb9fd201dc86297ca76ef14a324c4c424f2dcaa9b6486ffac9adaaccd64ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
